# Elementis Environment Variables
# Copy this file to .env and configure as needed

# =============================================================================
# HuggingFace Configuration (Optional)
# =============================================================================
# Token for accessing gated models or LangChain agent
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=

# Model for LangChain agent (tool calling)
# Default: meta-llama/Llama-3.2-3B-Instruct
HUGGINGFACE_MODEL=meta-llama/Llama-3.2-3B-Instruct

# =============================================================================
# Device Configuration
# =============================================================================
# Device for model inference: cuda, cpu, or auto
DEVICE=cuda

# =============================================================================
# ChromaDB Vector Database
# =============================================================================
# Path to ChromaDB storage
CHROMA_DB_PATH=./data/vector_db

# Collection name for document embeddings
CHROMA_COLLECTION_NAME=elementis_documents

# =============================================================================
# Embedding Model
# =============================================================================
# Model for document and query embeddings
# Default: sentence-transformers/all-MiniLM-L6-v2 (384 dim, multilingual)
# Alternative: BAAI/bge-m3 (1024 dim, better quality)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# LLM Configuration
# =============================================================================
# Model for final answer generation
# Default: microsoft/Phi-4 (14B, 4-bit quantized)
# Alternative: meta-llama/Llama-3.2-3B-Instruct
LLM_MODEL=microsoft/Phi-4

# Device for LLM (cuda recommended for 4-bit quantization)
LLM_DEVICE=cuda

# =============================================================================
# Router Model (Fixed in code)
# =============================================================================
# The router model is hardcoded to Qwen/Qwen2.5-1.5B-Instruct
# See src/agent.py for configuration

# =============================================================================
# Data Paths
# =============================================================================
# Path to source PDF documents
DOCUMENTS_PATH=./data/documents

# Path to processed markdown files
PROCESSED_PATH=./data/processed

# Path to vigilance data (floods, hotspots)
VIGILANCE_PATH=./data/vigilance

# =============================================================================
# RAG Pipeline Configuration
# =============================================================================
# Chunk size for document splitting (tokens)
CHUNK_SIZE=512

# Overlap between chunks (tokens)
CHUNK_OVERLAP=50

# Number of top results to retrieve
TOP_K_RESULTS=5

# =============================================================================
# Gradio Web Interface
# =============================================================================
# Port for Gradio server
GRADIO_SERVER_PORT=7860

# Share publicly via Gradio link (true/false)
GRADIO_SHARE=false

# =============================================================================
# Logging
# =============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
